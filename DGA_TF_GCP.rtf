{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 !pip install tldextract\
TF_ENABLE_ONEDNN_OPTS=0\
import numpy as np\
import tensorflow as tf\
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, LSTM, Bidirectional, Dropout, Attention\
from tensorflow.keras.models import Model\
from sklearn.model_selection import train_test_split\
from sklearn.preprocessing import LabelEncoder\
import csv\
import tldextract\
import pandas as pd\
\
def extract_primary_domain(url):\
    ext = tldextract.extract(url)\
    if not ext.subdomain:\
        return ext.domain\
    else:\
        return ext.subdomain\
\
dga_domains = pd.read_csv('gs://dga-dataset/dga_million.csv') # DGA-generated domain names\
legit_domains = pd.read_csv('gs://dga-dataset/majestic_million.csv')  # Legitimate domain names\
\
# Combine the datasets and create labels\
domains = dga_domains._append(legit_domains)\
\
\
domains = domains['Domain'].str.lower().apply(extract_primary_domain)\
domains = np.array(domains)\
labels = np.array(['DGA'] * len(dga_domains) + ['Legit'] * len(legit_domains))\
\
def extract_primary_domain(url):\
    ext = tldextract.extract(url)\
    return ext.domain\
\
# Combine the datasets and create labels\
\
\
domains = np.array(domains)\
labels = np.array(['DGA'] * len(dga_domains) + ['Legit'] * len(legit_domains))\
\
# Encode the labels\
label_encoder = LabelEncoder()\
labels = label_encoder.fit_transform(labels)\
\
# Tokenize domain names\
max_length = max([len(domain) for domain in domains])\
print(f"Max length is : \{max_length\}")\
vocab_size = 256  # ASCII characters\
tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\
tokenizer.fit_on_texts(domains)\
sequences = tokenizer.texts_to_sequences(domains)\
padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length)\
\
# Split the data into training and testing sets\
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\
\
# Build the Att-CNN-BiLSTM model\
inputs = Input(shape=(max_length,))\
embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length)(inputs)\
conv_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\
pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)\
flatten_layer = Flatten()(pooling_layer)\
reshape_layer = tf.keras.layers.Reshape(target_shape=(int(flatten_layer.shape[1]), 1))(flatten_layer)\
lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(reshape_layer)\
dropout_layer = Dropout(0.5)(lstm_layer)\
query = tf.keras.layers.Lambda(lambda x: x[:, -1:, :])(dropout_layer)\
value = lstm_layer\
attention_layer = Attention()([query, value])\
flatten_layer = Flatten()(attention_layer)\
output = Dense(1, activation='sigmoid')(flatten_layer)\
\
\
model = Model(inputs=inputs, outputs=output)\
model.summary()\
\
# Compile the model\
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\
\
\
# Train the model\
model.fit(X_train, y_train, epochs=5, batch_size=256, validation_split=0.1)\
\
# Evaluate the model\
loss, accuracy = model.evaluate(X_test, y_test)\
print(f"Test Accuracy: \{accuracy\}")\
\
# Save the model with input signatures\
@tf.function(input_signature=[tf.TensorSpec(shape=[None, max_length], dtype=tf.float32)])\
def serving_fn(inputs):\
    return model(inputs)\
\
tf.saved_model.save(model, 'gs://dga-dataset/my_model', signatures=\{'serving_default': serving_fn\})\
#model.save('gs://dga-dataset/my_model')}